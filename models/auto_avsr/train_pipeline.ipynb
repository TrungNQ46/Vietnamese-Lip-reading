{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you have a data structure like this:\n",
    "Data structure is base on the content in the split\n",
    "\n",
    "sneak peek at val.txt, we have:\n",
    "\n",
    "####################\n",
    "```\n",
    "data_output/Vlips_v1_video_non_audio\\BAIGI1\\BACHA2\\BACHA2_25.mp4\n",
    "data_output/Vlips_v1_video_non_audio\\BAIGI1\\BACHA2\\BACHA2_70.mp4\n",
    "data_output/Vlips_v1_video_non_audio\\BAIGI1\\BANHO3\\BANHO3_97.mp4\n",
    "...\n",
    "```\n",
    "####################\n",
    "\n",
    "\n",
    "so our data structure is like this:\n",
    "```\n",
    "Vlips/ \n",
    "  ├── split/\n",
    "  │   ├── test.txt\n",
    "  │   ├── train.txt\n",
    "  │   └── val.txt\n",
    "  └── data_output/\n",
    "        └── Vlips_v1_video_non_audio/\n",
    "            ├── BAIGI1/\n",
    "            |       ├── BACHA2/\n",
    "            |       |   ├── BACHA2_25.mp4\n",
    "            |       |   ├── BACHA2_25.csv\n",
    "            |       |   └── ...\n",
    "            |       └── BANHO3/\n",
    "            └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to create `<Vid ID>.txt` file and create train and test list (save in `data_split` folder). ***These txt files is different from the original txt files*** in the split folder, as they only contains the video ID., not the full path to the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting data to LRS2 structure: 100%|█████████| 5/5 [00:00<00:00, 297.72it/s]\n",
      "Converting data to LRS2 structure: 100%|█████████| 2/2 [00:00<00:00, 278.15it/s]\n",
      "Converting data to LRS2 structure: 100%|█████████| 2/2 [00:00<00:00, 274.69it/s]\n"
     ]
    }
   ],
   "source": [
    "!python create_lsr2_label.py --data-dir /home/aiot/notthinhbecamex/dnqh/Lip-Reading/data/Vlips/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess video data by cropping only the lips part and tokenize the text data.\n",
    "\n",
    "\n",
    "root_dir is the path to the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libEGL warning: DRI3: failed to query the version\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: DRI3: failed to query the version\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1728527561.460384 1146387 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "libEGL warning: DRI3: failed to query the version\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: DRI3: failed to query the version\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "Directory vili/labels created\n",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]W0000 00:00:1728527561.531466 1146398 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "100%|█████████████████████████████████████████████| 5/5 [00:14<00:00,  2.88s/it]\n"
     ]
    }
   ],
   "source": [
    "!python preparation/preprocess_lrs2lrs3.py \\\n",
    "    --data-dir /home/aiot/notthinhbecamex/dnqh/Lip-Reading/data/Vlips/ \\\n",
    "    --root-dir vili \\\n",
    "    --detector mediapipe \\\n",
    "    --dataset lrs2 \\\n",
    "    --subset train \\\n",
    "    --groups 1 \\\n",
    "    --job-index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libEGL warning: DRI3: failed to query the version\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: DRI3: failed to query the version\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1728527611.486650 1162818 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "libEGL warning: DRI3: failed to query the version\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: DRI3: failed to query the version\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "Directory vili/labels created\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]W0000 00:00:1728527611.618069 1162897 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:06<00:00,  3.28s/it]\n"
     ]
    }
   ],
   "source": [
    "!python preparation/preprocess_lrs2lrs3.py \\\n",
    "    --data-dir /home/aiot/notthinhbecamex/dnqh/Lip-Reading/data/Vlips/ \\\n",
    "    --root-dir vili \\\n",
    "    --detector mediapipe \\\n",
    "    --dataset lrs2 \\\n",
    "    --subset test \\\n",
    "    --groups 1 \\\n",
    "    --job-index 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libEGL warning: DRI3: failed to query the version\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: DRI3: failed to query the version\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1728527629.630549 1168240 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "libEGL warning: DRI3: failed to query the version\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "libEGL warning: DRI3: failed to query the version\n",
      "libEGL warning: MESA-LOADER: failed to open swrast: /usr/lib/dri/swrast_dri.so: cannot open shared object file: No such file or directory (search paths /usr/lib/x86_64-linux-gnu/dri:\\$${ORIGIN}/dri:/usr/lib/dri, suffix _dri)\n",
      "\n",
      "Directory vili/labels created\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]W0000 00:00:1728527629.689186 1168251 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:05<00:00,  2.67s/it]\n"
     ]
    }
   ],
   "source": [
    "!python preparation/preprocess_lrs2lrs3.py \\\n",
    "    --data-dir /home/aiot/notthinhbecamex/dnqh/Lip-Reading/data/Vlips/ \\\n",
    "    --root-dir vili \\\n",
    "    --detector mediapipe \\\n",
    "    --dataset lrs2 \\\n",
    "    --subset val \\\n",
    "    --groups 1 \\\n",
    "    --job-index 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model. Recommend gpu vram ~= 16GB. If no gpus meet the requirement, set trainer.gpus=0 and trainer.sync_batchnorm=false. data.max_frames have to be equal to the highest number of frames in 1 video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aiot/notthinhbecamex/dnqh/Lip-Reading/auto_avsr/train.py\", line 6, in <module>\n",
      "    from pytorch_lightning import seed_everything, Trainer\n",
      "  File \"/home/aiot/miniconda3/envs/auto_avsr/lib/python3.9/site-packages/pytorch_lightning/__init__.py\", line 20, in <module>\n",
      "    from pytorch_lightning.callbacks import Callback  # noqa: E402\n",
      "  File \"/home/aiot/miniconda3/envs/auto_avsr/lib/python3.9/site-packages/pytorch_lightning/callbacks/__init__.py\", line 14, in <module>\n",
      "    from pytorch_lightning.callbacks.base import Callback\n",
      "  File \"/home/aiot/miniconda3/envs/auto_avsr/lib/python3.9/site-packages/pytorch_lightning/callbacks/base.py\", line 26, in <module>\n",
      "    from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
      "  File \"/home/aiot/miniconda3/envs/auto_avsr/lib/python3.9/site-packages/pytorch_lightning/utilities/types.py\", line 25, in <module>\n",
      "    from torchmetrics import Metric\n",
      "  File \"/home/aiot/miniconda3/envs/auto_avsr/lib/python3.9/site-packages/torchmetrics/__init__.py\", line 24, in <module>\n",
      "    import scipy.signal\n",
      "  File \"/home/aiot/miniconda3/envs/auto_avsr/lib/python3.9/site-packages/scipy/signal/__init__.py\", line 330, in <module>\n",
      "    from ._peak_finding import *\n",
      "  File \"/home/aiot/miniconda3/envs/auto_avsr/lib/python3.9/site-packages/scipy/signal/_peak_finding.py\", line 1201, in <module>\n",
      "    def find_peaks_cwt(vector, widths, wavelet=None, max_distances=None,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py exp_dir=ckpt \\\n",
    "                exp_name=cstm \\\n",
    "                data.modality=video \\\n",
    "                data.dataset.root_dir=cstm_preprocessed \\\n",
    "                data.dataset.train_file=lrs2_train_transcript_lengths_seg24s.csv \\\n",
    "                data.dataset.val_file=lrs2_test_transcript_lengths_seg24s.csv \\\n",
    "                data.max_frames=387"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No gpu command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aiot/miniconda3/envs/auto_avsr/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1584: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "/home/aiot/miniconda3/envs/auto_avsr/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:432: UserWarning: The number of training samples (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Epoch 0: 100%|█████████████████| 5/5 [00:51<00:00, 10.37s/it, loss=421, v_num=3]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|█████████████████| 5/5 [01:18<00:00, 15.64s/it, loss=421, v_num=3]\u001b[A\n",
      "Epoch 1: 100%|█████████████████| 5/5 [00:56<00:00, 11.37s/it, loss=349, v_num=3]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████████████| 5/5 [01:24<00:00, 16.80s/it, loss=349, v_num=3]\u001b[A\n",
      "Epoch 1: 100%|█████████████████| 5/5 [01:24<00:00, 16.81s/it, loss=349, v_num=3]\u001b[A^C\n"
     ]
    }
   ],
   "source": [
    "!python train.py exp_dir=ckpt \\\n",
    "                exp_name=vili \\\n",
    "                data.modality=video \\\n",
    "                data.dataset.root_dir=vili \\\n",
    "                data.dataset.train_file=lrs2_train_transcript_lengths_seg24s.csv \\\n",
    "                data.dataset.val_file=lrs2_test_transcript_lengths_seg24s.csv \\\n",
    "                data.max_frames=387 \\\n",
    "                trainer.max_epochs=10 \\\n",
    "                trainer.gpus=0 trainer.sync_batchnorm=false"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_avsr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
